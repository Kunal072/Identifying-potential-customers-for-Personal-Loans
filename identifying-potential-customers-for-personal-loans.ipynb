{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1286598,"sourceType":"datasetVersion","datasetId":742843}],"dockerImageVersionId":29962,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Assignment - Supervised Learning","metadata":{}},{"cell_type":"markdown","source":"### Objective","metadata":{}},{"cell_type":"markdown","source":"Data Analysis to identify the potential customers who have a higher probability of purchasing the loan.\n","metadata":{}},{"cell_type":"markdown","source":"# Context","metadata":{}},{"cell_type":"markdown","source":"\nThis case is about a bank (Thera Bank) whose management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors). A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio with minimal budget.\n\nThe department wants to build a model that will help them identify the potential customers who have a higher probability of purchasing the loan. This will increase the success ratio while at the same time reduce the cost of the campaign.","metadata":{}},{"cell_type":"markdown","source":"# Attribute Information: Column descriptions","metadata":{}},{"cell_type":"markdown","source":"\n\n    ID                    :Customer ID \n\n    Age                   :Customer's age in completed years \n\n    Experience            :#years of professional experience \n\n    Income                :Annual income of the customer ($000) \n\n    ZIPCode               :Home Address ZIP code \n\n    Family                :Family size of the customer \n\n    CCAvg                 :Avg. spending on credit cards per month ($000) \n\n    Education             :Education Level. 1: Undergrad; 2: Graduate; 3: Advanced/Professional \n\n    Mortgage              :Value of house mortgage if any. ($000) \n\n    Personal Loan         :Did this customer accept the personal loan offered in the last campaign? \n\n    Securities Account    :Does the customer have a securities account with the bank? \n\n    CD Account            :Does the customer have a certificate of deposit (CD) account with the bank? \n\n    Online                :Does the customer use internet banking facilities? CreditCard Does the customer \n                       uses a credit card issued  by UniversalBank?","metadata":{}},{"cell_type":"markdown","source":"### Acknowledgements","metadata":{}},{"cell_type":"markdown","source":"This data set was given as part of course in machine learning. I have also added my observations on the data. I thank Great Learning and my faculty for giving an opportunity to work on this dataset.","metadata":{}},{"cell_type":"markdown","source":"### Inspiration","metadata":{}},{"cell_type":"markdown","source":"Study the data distribution in each attribute, share my findings. Used a classification model to predict the likelihood of a liability customer buying personal loans.","metadata":{}},{"cell_type":"markdown","source":"### Technologies & Libraries","metadata":{}},{"cell_type":"markdown","source":"Python3\n\n    Logistic Regression ; KNN Classifier ; Naive Bayes ; SVM ; Metrics ; Preprocessing ; \n\n    Pandas ; NumPy ; Matplotlib ; SeaBorn ; SKLearn ; SciPy ; Statsmdels ; Copy ; OS ; re ; traceback ; string , Scikitplot;\n    \n    \n        \n        \n\n","metadata":{}},{"cell_type":"code","source":"## Necessary libraries are imported\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\nimport os\nimport statsmodels.api as sm\nimport scipy.stats as stats\nimport copy\nimport pandas.core.algorithms as algos\nfrom pandas import Series\nimport re\nimport traceback\nimport string\nimport scikitplot as skplt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc\nfrom scipy.stats import zscore\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn import svm\nfrom sklearn.preprocessing import MinMaxScaler\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom yellowbrick.model_selection import FeatureImportances","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n##### Dataset\n\nBankLoan.csv\n","metadata":{}},{"cell_type":"code","source":"print(os.listdir(\"../input\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset is read \nbank_per_loan_df = pd.read_csv('../input/Bank_Personal_Loan_Modelling(1).csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_per_loan_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pandas Profiling","metadata":{}},{"cell_type":"markdown","source":"I have attached the pandas profiling html file separate, it gave me error in jupyter notebook, due to version issues. So did it in Google Colab and generated the report. It has been given as separate html file in the submission.\n\nCode used in Colab:\n\n !pip install -U pandas-profiling\n \n import pandas_profiling as pp\n \n profile= pp.ProfileReport(bank_per_loan_df)\n \n profile.to_file('./output.html') \n","metadata":{}},{"cell_type":"markdown","source":"##### Points observed by profile report & univariate analysis:","metadata":{}},{"cell_type":"markdown","source":"    The data set got 0 missing cells.\n\n    It got 7 numeric variables: ‘Age’, ‘CC_Avg’, ‘ID’, ‘Income’, ‘Mortgage’, ‘Zip_Code’, ‘Experience’\n    It got 2 categorical variables: ‘Education’, ‘Family’\n    It got 5 Boolean variables: ‘CD_Account’, ‘Credit_Card’, ‘Online’, ‘Personal_Loan’, ‘Securities Account’\n    Personal Loan is highly correlated with Income, average spending on Credit cards, mortgage & if the customer has a certificate of deposit (CD) account with the bank.\n    Also, Experience is highly correlated with Age (ρ = 0.994214857)\n\n##### Categorical\n    42% of the candidates are graduated, while 30% are professional and 28% are Undergraduate.\n    Around 29% of the customer’s family size is 1.\n\n##### Boolean\n    94% of the customer does not have a certificate of deposit (CD) account with the bank.\n    Around 71% of the customer does not use a credit card issued by Universal Bank.\n    Around 60% of customers use internet banking facilities.\n    Around 90% of the customer does not accept the personal loan offered in the last campaign.\n    Around 90% of the customer does not have a securities account with the bank.\n    \n##### Numeric\n    The mean age of the customers is 45 with standard deviation of 11.5. Also, we had estimated the average age in hypothesis testing between 30–50. The curve is slightly negatively skewed (Skewness = -0.02934068151) hence the curve is fairly symmetrical\n    The mean of Avg. spending on credit cards per month is 1.93 with standard deviation of 1.75. The curve is highly positive skewed (Skewness = 1.598443337)\n    The mean annual income of the customer is 73.77 with standard deviation of 46. The curve is moderately positive skewed (Skewness = 0.8413386073)\n    The mean value of house mortgage is 56.5 with standard deviation of 101.71. The curve is highly positive skewed (Skewness = 2.104002319) and there are a lot of outlier’s present (Kurtosis = 4.756796669)\n    \n    \n    Also, no need for ‘ID’, ‘ZIP_Code’ & ‘Experience’ columns for further analysis since ‘ID’ and ‘ZIP_Code’ are just numbers of series & ‘Experience’ is highly correlated with ‘Age’.\n","metadata":{}},{"cell_type":"markdown","source":"#### Variables definition","metadata":{}},{"cell_type":"markdown","source":"\n\n    ID - Customer ID\n    Age - Customer's age in completed years\n    Experience - Number of years of professional experience.\n    Income - Annual income of the customer (in $ 1000).\n    ZIPCode - Home Address ZIP code.\n    Family - Family size of the customer\n    CCAvg - Avg. spending on credit cards per month - in thousands usd\n    Education - Education Level. 1: Undergrad; 2: Graduate; 3: Advanced/Professional\n    Mortgage - Value of house mortgage if any - in thousands usd\n    Personal Loan - Did this customer accept the personal loan offered in the last campaign?\n    Securities Account - Does the customer have a securities account with the bank?\n    CD Account - Does the customer have a certificate of deposit (CD) account with the bank?\n    Online - Does the customer use internet banking facilities?\n    CreditCard - Does the customer uses a credit card issued by UniversalBank?\n\n","metadata":{}},{"cell_type":"markdown","source":"##### Categorical Feature:","metadata":{}},{"cell_type":"markdown","source":"\n    Family\n    Education\n    ID\n    Zip Code\n    Securities Account\n    CD Account\n    Online\n    Credit Card","metadata":{}},{"cell_type":"markdown","source":"##### Numerical feature:","metadata":{}},{"cell_type":"markdown","source":"    Age\n    Experience\n    Income\n    CCAvg\n    Mortgage","metadata":{}},{"cell_type":"markdown","source":"##### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Question 1: Read the column description and ensure you understand each attribute well","metadata":{}},{"cell_type":"code","source":"print(bank_per_loan_df.columns)\nprint(bank_per_loan_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation","metadata":{}},{"cell_type":"markdown","source":"We have 13 independent variables and 1 dependent variable i.e. ‘Personal Loan’ in the data set. Also, we got 5000 rows which can be split into test & train datasets.","metadata":{}},{"cell_type":"code","source":"bank_per_loan_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation","metadata":{}},{"cell_type":"markdown","source":"    No Missing Values","metadata":{}},{"cell_type":"code","source":"bank_per_loan_df.isna().apply(pd.value_counts)   #null value check","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation","metadata":{}},{"cell_type":"markdown","source":"    No Null Values","metadata":{}},{"cell_type":"markdown","source":"###### Check for duplicate records","metadata":{}},{"cell_type":"code","source":"bank_per_loan_df.duplicated().any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\n\n\nno duplicate records","metadata":{}},{"cell_type":"code","source":"bank_per_loan_df.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation","metadata":{}},{"cell_type":"markdown","source":"    Column 'Experience' has negative values\n    \n    Binary variables \"Personal Loan\", \"CreditCard\", \"Online\", \"CD Account\", \"Securities Account\" has clean data\n    \n    Ordinary Cat variables \"Family\" and \"Education\" are clean too","metadata":{}},{"cell_type":"markdown","source":"Replacing the negative values with the mean value of the column","metadata":{}},{"cell_type":"code","source":"any(bank_per_loan_df['Experience'] < 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_med = bank_per_loan_df.loc[:,\"Experience\"].median()\nprint(\" Experience median is\", exp_med)\nbank_per_loan_df.loc[:, 'Experience'].replace([-1, -2, -3], [exp_med, exp_med, exp_med], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"any(bank_per_loan_df['Experience'] < 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_per_loan_df.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Question2: Study the data distribution in each attribute, share your findings. ","metadata":{}},{"cell_type":"markdown","source":"##### Finding: ( analysis is shown below)\n\n1). ID: This attribute can be dropped.Though the data distribution is normal.\n\n2). Age:Three small peaks can be indicating three values of age would be slightly more in number.However, the mean and median of the attribute is equal.The distribution is in considerable shape.\n\n3). Eductaion : Mean and median is almost equal. Data is finely distributed. A few peaks shows different values dominance.\n\n4). Income : We can clearly see data is highly left skewed.Data for less income customers is more in the sample.\n\n5). ZIP Code: The attribute has sharp peaks telling the data from particular places are collected more.Spread is also less in the sample. More data from different places can be collected.\n\n6).Family: It has 4 peaks(4 values) , families with least member is highest in the sample.\n\n7).Mortage: This attribute is highly left skewed with a very high peak on the left telling us that most customer are having least mortage while a very few have some mortage.\n\n8).Securities Account : This attributes tells us that majorly cutomers are not having Security account.\n\n9).CD account: Most of the customers dont have CDaccounts.\n\n10).Online: Higher number of customers use online banking in the sample.\n\n11).Credit Card: This attribute has less customers using CC in comparison to the CC users.\n","metadata":{}},{"cell_type":"markdown","source":"# Univariate Analysis of the continuous variables - 1","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize= (40.5,40.5))\nplt.subplot(5,3,1)\nplt.hist(bank_per_loan_df.Age, color='lightpink', edgecolor = 'black')\nplt.xlabel('Age')\n\nplt.subplot(5,3,2)\nplt.hist(bank_per_loan_df.Experience, color='darkblue', edgecolor = 'black')\nplt.xlabel('Experience')\n\nplt.subplot(5,3,3)\nplt.hist(bank_per_loan_df.Income, color='lightblue', edgecolor = 'black')\nplt.xlabel('Income')\n\nplt.subplot(5,3,4)\nplt.hist(bank_per_loan_df.CCAvg, color='lightgreen', edgecolor = 'black')\nplt.xlabel('Credit Card Average')\n\nplt.subplot(5,3,5)\nplt.hist(bank_per_loan_df.Mortgage, color='purple', edgecolor = 'black')\nplt.xlabel('Mortgage')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation","metadata":{}},{"cell_type":"markdown","source":"    Age & Experience seems to be quiet normally distributed\n\n    Income, CC Average & Mortgage are highly skewed\n\n","metadata":{}},{"cell_type":"code","source":"# Checking for Skewness of data\nSkewness = pd.DataFrame({'Skewness' : [stats.skew(bank_per_loan_df.Age),stats.skew(bank_per_loan_df.Experience),stats.skew(bank_per_loan_df.Income),stats.skew(bank_per_loan_df.CCAvg)\n                                      ,stats.skew(bank_per_loan_df.Mortgage)]},index=['Age','Experience','Income','CCAvg','Mortgage'])\nSkewness","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation","metadata":{}},{"cell_type":"markdown","source":"    Age and Experience seems to be quiet Symmetrical\n\n    Income, CCAvg and Mortgage are Positively skewed, as they are highly skewed there will be quiet a lot of extreme values","metadata":{}},{"cell_type":"markdown","source":"# Univariate Analysis of the continuous variables - 2","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize= (25,25))\nplt.subplot(5,2,1)\nsns.boxplot(x= bank_per_loan_df.Age, color='yellow')\n\nplt.subplot(5,2,2)\nsns.boxplot(x= bank_per_loan_df.Experience, color='red')\n\nplt.subplot(5,2,3)\nsns.boxplot(x= bank_per_loan_df.Income, color='lightgreen')\n\nplt.subplot(5,2,4)\nsns.boxplot(x= bank_per_loan_df.CCAvg, color='lightpink')\n\nplt.subplot(5,2,5)\nsns.boxplot(x= bank_per_loan_df.Mortgage, color='lightblue')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Inference","metadata":{}},{"cell_type":"markdown","source":"    Age feature is normally distributed with majority of customers falling between 35 years and 55 years of age. We can infer from the boxplot above, and also in info attained from describe() shows mean is almost equal to median.\n\n    Experience is normally distributed with more customer having experience starting from 11 years to 30 Years. Here also the mean is equal to median.\n\n    Income is positively skewed. Majority of the customers have income between 45K and 55K. We can confirm this by saying the mean is greater than the median.\n\n    CCAvg is also a positively skewed variable and average spending is between 0K to 10K and majority spends less than 2.5K.\n\n    Mortgage 70% of the individuals have a mortgage of less than 40K. However the max value is 635K.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Univariate Analysis of the categorical variables","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(30,45))\n\nplt.subplot(6,2,1)\nbank_per_loan_df['Family'].value_counts().plot(kind=\"bar\", align='center',color = 'green',edgecolor = 'black')\nplt.xlabel(\"Number of Family Members\")\nplt.ylabel(\"Count\")\nplt.title(\"Family Members Distribution\")\n\nplt.subplot(6,2,2)\nbank_per_loan_df['Education'].value_counts().plot(kind=\"bar\", align='center',color = 'blue',edgecolor = 'black')\nplt.xlabel('Level of Education')\nplt.ylabel('Count ')\nplt.title('Education Distribution')\n\nplt.subplot(6,2,3)\nbank_per_loan_df['Securities Account'].value_counts().plot(kind=\"bar\", align='center',color = 'red',edgecolor = 'black')\nplt.xlabel('Holding Securities Account')\nplt.ylabel('Count')\nplt.title('Securities Account Distribution')\n\nplt.subplot(6,2,4)\nbank_per_loan_df['CD Account'].value_counts().plot(kind=\"bar\", align='center',color = 'lightpink',edgecolor = 'black')\nplt.xlabel('Holding CD Account')\nplt.ylabel('Count')\nplt.title(\"CD Account Distribution\")\n\nplt.subplot(6,2,5)\nbank_per_loan_df['Online'].value_counts().plot(kind=\"bar\", align='center',color = 'lightgreen',edgecolor = 'black')\nplt.xlabel('Accessing Online Banking Facilities')\nplt.ylabel('Count')\nplt.title(\"Online Banking Distribution\")\n\nplt.subplot(6,2,6)\nbank_per_loan_df['CreditCard'].value_counts().plot(kind=\"bar\", align='center',color = 'yellow',edgecolor = 'black')\nplt.xlabel('Holding Credit Card')\nplt.ylabel('Count')\nplt.title(\"Credit Card Distribution\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations","metadata":{}},{"cell_type":"markdown","source":"    The variables family and education are ordinal variables. The distribution of families is evenly distributed\n    It seems that many of the population is not holding Securities Account and CD Account, vast difference is visible","metadata":{}},{"cell_type":"code","source":"#Pairplot\nsns.pairplot(bank_per_loan_df.iloc[:,1:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation","metadata":{}},{"cell_type":"markdown","source":"    'Age' has an association with 'Experience\n    \n    Age feature is normally distributed with majority of customers falling between 30 years and 60 years of age. We can confirm this by looking at the describe statement above, which shows mean is almost equal to median\n\n\n    Experience is normally distributed with more customer having experience starting from 8 years. Here the mean is equal to median. There are negative values in the Experience. This could be a data input error as in general it is not possible to measure negative years of experience. We can delete these values, because we have 3 or 4 records from the sample.\n\n\n    Income is positively skewed. Majority of the customers have income between 45K and 55K. We can confirm this by saying the mean is greater than the median\n\n    CCAvg is also a positively skewed variable and average spending is between 0K to 10K and majority spends less than 2.5K\n\n    Mortgage 70% of the individuals have a mortgage of less than 40K. However the max value is 635K\n\n    The variables family and education are ordinal variables. The distribution of families is evenly distributes","metadata":{}},{"cell_type":"markdown","source":"# Dependant variable analysis","metadata":{}},{"cell_type":"code","source":"bank_per_loan_df[\"Personal Loan\"].value_counts().to_frame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.value_counts(bank_per_loan_df[\"Personal Loan\"]).plot(kind=\"bar\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bivariate Analysis","metadata":{}},{"cell_type":"markdown","source":"Hypotheses based on the data and loan awarness:\n\n    -High salaries are less feasible to buy personal loans while customers with medium or low salaries are more feasible for buying personal loans.\n    \n    \n    -More the number of earning family members, less probability of buying personal loans.\n    \n    \n    -Customers with probably the age of 30–50 will buy personal loans.\n    \n    \n    -The customer is a graduate or under-graduate can affect the buying probability, people who are graduated or Advanced Professionals are more viable to buy personal loans from a bank rather than people who are under-graduated.","metadata":{}},{"cell_type":"markdown","source":"###### Categorical Independent Variable vs Target Variable","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='Personal Loan',hue='Education',data=bank_per_loan_df)\ntable=pd.crosstab(bank_per_loan_df['Education'],bank_per_loan_df['Personal Loan'])\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\nplt.title('Stacked Bar Chart of Education vs Purchase')\nplt.xlabel('Education')\nplt.ylabel('Proportion of Loans')\n# undergraduate has very less prob of taking the loan","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"education = pd.crosstab(bank_per_loan_df['Education'],bank_per_loan_df['Personal Loan'])\neducation.div(education.sum(1).astype(float),axis =0).plot(kind='bar',stacked=True)\nprint('cross tabulation can be given as : ', '\\n', education)\nprint('cross tabulation can be given in percentage as : ', '\\n', education.div(education.sum(1).astype(float),axis =0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation","metadata":{}},{"cell_type":"markdown","source":"From the above plots, we can infer that customers who are more educated have a higher probability of buying personal loans. Hence our hypothesis was true…!","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='Family',data=bank_per_loan_df,hue='Personal Loan',palette='Set1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"family = pd.crosstab(bank_per_loan_df['Family'],bank_per_loan_df['Personal Loan'])\nfamily.div(family.sum(1).astype(float),axis =0).plot(kind='bar',stacked=True)\nprint('cross tabulation can be given as : ', '\\n', family)\nprint('cross tabulation can be given in percentage as : ', '\\n', family.div(family.sum(1).astype(float),axis =0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation","metadata":{}},{"cell_type":"markdown","source":"The number of family members not significantly affect probability. Hence it contradicts our hypothesis that the number of family members will affect the probability.","metadata":{}},{"cell_type":"markdown","source":"# Influence of income and education on personal loan","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x='Education',y='Income',hue='Personal Loan',data=bank_per_loan_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation : \n    \n    \n    \n    \n    It seems the customers whose education level is 1 is having more income. However customers who has taken the personal loan have the same income levels","metadata":{}},{"cell_type":"markdown","source":"# Influence of mortage and education on personal loan","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x=\"Education\", y='Mortgage', hue=\"Personal Loan\", data=bank_per_loan_df,color='pink')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\n\nFrom the above chart it seems that customer who do not have personal loan and customer who has personal loan have high mortgage","metadata":{}},{"cell_type":"markdown","source":"# Boolean Independent Variable vs Target Variable","metadata":{}},{"cell_type":"markdown","source":"Let us now look at the Boolean variables (‘CD_Account’, ‘Credit_Card’, ‘Online’, ‘Securities Account’) vs Target variable (‘Personal_Loan’)","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='CD Account',data=bank_per_loan_df,hue='Personal Loan')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CD_Account = pd.crosstab(bank_per_loan_df['CD Account'],bank_per_loan_df['Personal Loan'])\nCD_Account.div(CD_Account.sum(1).astype(float),axis =0).plot(kind='bar',stacked=True)\nprint('cross tabulation can be given as : ', '\\n', CD_Account)\nprint('cross tabulation can be given in percentage as : ', '\\n', CD_Account.div(CD_Account.sum(1).astype(float),axis =0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation","metadata":{}},{"cell_type":"markdown","source":"The customer who has a certificate of deposit (CD) account with the bank seems to buy personal loans from the bank.","metadata":{}},{"cell_type":"markdown","source":"##### Let us now compare between the personal loan buyers who use or doesn’t use a credit card issued by UniversalBank","metadata":{}},{"cell_type":"code","source":"credit = pd.crosstab(bank_per_loan_df['CreditCard'],bank_per_loan_df['Personal Loan'])\ncredit.div(credit.sum(1).astype(float),axis =0).plot(kind='bar',stacked=True)\nprint('cross tabulation can be given as : ', '\\n', credit)\nprint('cross tabulation can be given in percentage as : ', '\\n', credit.div(credit.sum(1).astype(float),axis =0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\n\nThe customer who uses or doesn’t use a credit card issued by UniversalBank doesn’t seem to affect the probability of buying a personal loan.","metadata":{}},{"cell_type":"markdown","source":"##### Let us now compare the personal loan buyer’s customer who uses or doesn’t use internet banking facilities:","metadata":{}},{"cell_type":"code","source":"online = pd.crosstab(bank_per_loan_df['Online'],bank_per_loan_df['Personal Loan'])\nonline.div(online.sum(1).astype(float),axis =0).plot(kind='bar',stacked=True)\nprint('cross tabulation can be given as : ', '\\n', online)\nprint('cross tabulation can be given in percentage as : ', '\\n', online.div(online.sum(1).astype(float),axis =0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Observation\n\nThe customer who uses or doesn’t use internet banking facilities seems to not affect the probability of buying personal loans.","metadata":{}},{"cell_type":"markdown","source":"##### Let us now compare between the personal loan buyer’s customer who has or doesn’t have a securities account with the bank:","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=\"Securities Account\", data=bank_per_loan_df,hue=\"Personal Loan\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"securities = pd.crosstab(bank_per_loan_df['Securities Account'],bank_per_loan_df['Personal Loan'])\nsecurities.div(securities.sum(1).astype(float),axis =0).plot(kind='bar',stacked=True)\nprint('cross tabulation can be given as : ', '\\n', securities)\nprint('cross tabulation can be given in percentage as : ', '\\n', securities.div(securities.sum(1).astype(float),axis =0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Obervations\n\nThe customers who have or don’t have securities account with the bank do not affect the probability of buying a personal loan.","metadata":{}},{"cell_type":"markdown","source":"# Influence of few attributes on 'Personal Loan' - Dependant Variable","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(y = 'Income', x = 'Age', data = bank_per_loan_df, hue = 'Personal Loan')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\n\nplt.subplot(3,1,1)\nsns.scatterplot(bank_per_loan_df.CCAvg, bank_per_loan_df.Income, hue = bank_per_loan_df['Personal Loan'], palette= ['lightpink','green'])\n\nplt.subplot(3,1,2)\nsns.scatterplot(bank_per_loan_df.Family, bank_per_loan_df.Income, hue = bank_per_loan_df['Personal Loan'], palette= ['lightblue','purple'])\n\nplt.subplot(3,1,3)\nsns.scatterplot(bank_per_loan_df.Income, bank_per_loan_df.Mortgage, hue = bank_per_loan_df['Personal Loan'], palette= ['lightgreen','green'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation","metadata":{}},{"cell_type":"markdown","source":"    The graph show persons who have personal loan have a higher credit card average.\n\n    It is clearly visible that as the members of family increases (say >=3) the necessity of loan is also increasing.\n\n    It is very precise that as the income increases (approx 100K) the mortgage value also increases gradually wiht the necessity of personal loan.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\n\nplt.subplot(3,1,1)\nsns.scatterplot(bank_per_loan_df.Age, bank_per_loan_df.Experience, hue = bank_per_loan_df['Personal Loan'], palette= ['lightpink','green'])\n\nplt.subplot(3,1,2)\nsns.scatterplot(bank_per_loan_df.Education, bank_per_loan_df.Income, hue = bank_per_loan_df['Personal Loan'], palette= ['lightgreen','green'])\n\nplt.subplot(3,1,3)\nsns.scatterplot(bank_per_loan_df.Education, bank_per_loan_df.Mortgage, hue = bank_per_loan_df['Personal Loan'], palette= ['red','green'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation","metadata":{}},{"cell_type":"markdown","source":"    'Age' has a very strong association with 'Experience' but nothing gets affected with loan attribute.\n    It seems that customers with education level is 1 is having more income which is mere equal to the customers who has taken the personal loan.\n    Customers with education level 2 & 3 seems to take personal loan as they have high mortgage.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\n\nplt.subplot(2,2,1)\nsns.countplot(x=\"Securities Account\", data=bank_per_loan_df ,hue=\"Personal Loan\")\n\nplt.subplot(2,2,2)\nsns.countplot(x='CD Account' ,data=bank_per_loan_df ,hue='Personal Loan')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation","metadata":{}},{"cell_type":"markdown","source":"    Majority of customers who does not have loan is holding securities account, Whereas small proportion of customers having loan does hold but majority of them do not have securities account.\n    Customers who does not have CD account, does not have loan as well, but almost all customers who has CD account has loan as well","metadata":{}},{"cell_type":"code","source":"sns.distplot(bank_per_loan_df[bank_per_loan_df[\"Personal Loan\"] == 0]['Income'], color = 'b')\nsns.distplot(bank_per_loan_df[bank_per_loan_df[\"Personal Loan\"] == 1]['Income'], color = 'y')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation","metadata":{}},{"cell_type":"markdown","source":"    The graph show those who have personal loan also have a higher income.","metadata":{}},{"cell_type":"code","source":"sns.distplot( bank_per_loan_df[bank_per_loan_df['Personal Loan'] == 0]['CCAvg'], color = 'r')\nsns.distplot( bank_per_loan_df[bank_per_loan_df['Personal Loan'] == 1]['CCAvg'], color = 'g')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Credit card spending of Non-Loan customers: ',bank_per_loan_df[bank_per_loan_df['Personal Loan'] == 0]['CCAvg'].median()*1000)\nprint('Credit card spending of Loan customers    : ', bank_per_loan_df[bank_per_loan_df['Personal Loan'] == 1]['CCAvg'].median()*1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation: \n\nThe graph show persons who have personal loan have a higher credit card average. Average credit card spending with a median of 3800 dollar indicates a higher probability of personal loan. Lower credit card spending with a median of 1400 dollars is less likely to take a loan. This could be useful information.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.relplot(x=\"Income\", y=\"CCAvg\" ,aspect = 2 ,data=bank_per_loan_df)\nplt.show()\nplt.clf()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\n\nIncome and credit card average use is also related in a linear fashion and is more dense in the income bracket of 50k-100k bracket.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\ncolors = {1:'red',2:'yellow',3:'green'}\nax.scatter(bank_per_loan_df['Experience'],bank_per_loan_df['Age'],c=bank_per_loan_df['Education'].apply(lambda x:colors[x]))\nplt.xlabel('Experience')\nplt.ylabel('Age')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation:\n\nThe above plot show with experience and age have a positive correlation. As experience increase age also increases. Also the colors show the education level. There is gap in the mid forties of age and also more people in the under graduate level","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.relplot(x=\"Income\", y=\"Mortgage\", #hue=\"Personal Loan\",\n            col=\"Education\", data=bank_per_loan_df)\nplt.show()\nplt.clf()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Observation\n\n\nHigher income level and higher education level customers have very few mortgages on them. Plus there are some smart people who dont have any mortgages across all education levels.\n\nThe mortgages are mainly concentrated between 0k-80k annual income individuals irrespective of the education background.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nsns.relplot(x=\"Experience\", y=\"Income\",col = \"Education\",\n             data=bank_per_loan_df)\nplt.show()\nplt.clf()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation:\n\nIncome of customers with higher experiences is an even spread and not related linearly.\n\nIt should be noted that even with Bachelors level education, higher experience indivudals have higher income as compared to their more educated counterparts.\n\nThe scatter plot is sparse above 100k USD for higher educated customers with the same experience.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nsns.relplot(x=\"Income\", y=\"Mortgage\",col = \"Family\",# hue=\"Education\",\n             data=bank_per_loan_df)\nplt.show()\nplt.clf()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\n\nIncome and Mortgage was linearly related.\n\nWhen we see this relation with respect to the number of family members we see that above 100k USD annual income, families of 3 and 4 have lesser mortgages as compared to families of 1 or 2.","metadata":{}},{"cell_type":"code","source":"melted_data = pd.melt(bank_per_loan_df.iloc[:,9:])\nmelted_data.loc[melted_data['value'] == 0 , ['value']] = 'No'\nmelted_data.loc[melted_data['value'] == 1 , ['value']] = 'Yes'\nplt.figure(figsize=(15,5))\nsns.countplot(x=\"variable\", hue=\"value\", data=melted_data)\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\nFrom the above graph we can observe that a major portion of the customers have no securities or CD Accounts.\n\nAround 3000 use Online banking but not many use credit cards.\n\nCredit cards usage is mostly among youngsters generally.\n\nThe average age of this dataset is 45, so it makes sense that credit card users are less.","metadata":{}},{"cell_type":"code","source":"numerical_1 = ['Age' , 'Experience' ,'Family' ,'Income' , 'CCAvg' , 'Mortgage']\nfig, ax_1 = plt.subplots(2, 3, figsize=(15, 10))\nfor var_1, subplot in zip(numerical_1, ax_1.flatten()):\n    sns.boxplot(x='Personal Loan', y=var_1, data=bank_per_loan_df, ax=subplot)\nplt.show()\nplt.clf()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our EDA can conclude with analysis of the numerical values with a categorical Personal Loan feature, and a box plot is the best way to do it\n\nAge of the customer is not a defining factor if the person will accept a personal loan or not.\nProfessional years of experience also not a governing factor.\n\nAs we saw in the previous graph, a family of 3 or 4 has lesser mortgages even with higher incomes. The reason, based on this box plot can be the fact that taking a personal loan with higher interest rate might seem justified to them. Will show it in a graph below.\n\nAs expected, higher the income more is the chance that a person will accept a personal loan offer from the bank.\n\nIf one's credit card average spending per month is higher, they will probably accept a personal loan offer.\nHigher mortgage means a custome might accept a personal loan offer.","metadata":{}},{"cell_type":"markdown","source":"# Checking for correlation","metadata":{}},{"cell_type":"code","source":"# Correlation with heat map\ncorr_overall = bank_per_loan_df.corr()\nsns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 2.5})\nplt.figure(figsize=(13,7))\n# create a mask so we only see the correlation values once\nmask = np.zeros_like(corr_overall)\nmask[np.triu_indices_from(mask, 1)] = True\na = sns.heatmap(corr_overall,mask=mask, annot=True, fmt='.2f')\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation=30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Observation\n\nIncome and CCAvg is moderately correlated.\n\nAge and Experience is highly correlated","metadata":{}},{"cell_type":"code","source":"ncol_2 = ['Age', 'Income','CCAvg', 'Mortgage']\ngrid = sns.PairGrid(bank_per_loan_df, y_vars = 'Experience', x_vars = ncol_2, height = 4)\ngrid.map(sns.regplot);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation:\n\nAge' has a very strong association with 'Experience\n\nIs there some association between personal characteristics and the fact that person obtained Personal Loan?\n\nLet's check what the values or group of values of each variable lies inside group that have 'Personal Loan' and don't have that.\n\nSince we found strong association between 'Age' and 'Experience' we decided to exclud 'Experience' from analysis steps to avoid multicollinearity.","metadata":{}},{"cell_type":"markdown","source":"##### QUANTATIVE VARIABLES\n\n['Age', 'Income', 'CCAvg', 'Mortgage'] with Personal Loan","metadata":{}},{"cell_type":"code","source":"quant_df=bank_per_loan_df[['Personal Loan', 'Age', 'Income', 'CCAvg', 'Mortgage']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_per_loan_df[['Personal Loan', 'Age', 'Income', 'CCAvg', 'Mortgage']].corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(bank_per_loan_df[['Personal Loan', 'Age', 'Income', 'CCAvg', 'Mortgage']].corr(), annot = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_per_loan_df[['Personal Loan', 'Age', 'Income', 'CCAvg', 'Mortgage']].corr()['Personal Loan'][1:].plot.bar()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation","metadata":{}},{"cell_type":"markdown","source":"    The above diagram shows a clear vision on the correlation between the independant variable and dependant variables, we see that 'Income' and 'Credit Card Average' has some correlation with 'Personal Loan'.","metadata":{}},{"cell_type":"markdown","source":"##### Let's check our confidense about this statment with logistic regression model:","metadata":{}},{"cell_type":"code","source":"quant_df['intercept'] = 1\nlog_mod_check = sm.Logit(quant_df['Personal Loan'], quant_df[['intercept', 'Age', 'Income', 'CCAvg', 'Mortgage']]).fit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_mod_check.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# include 'intercept'\nlog_mod_check.pvalues[1:5].plot.bar()\nplt.axhline(y = 0.05);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\n\nWe can say with confidence that 'Income' and 'CCAvg' both has statisticaly significant association with 'Personal Loan', since their p-value in logistic regression < 0.05","metadata":{}},{"cell_type":"markdown","source":"###### The bar chart of coefficient distribution","metadata":{}},{"cell_type":"code","source":"# exclude 'intercept'\nlog_mod_check.params[1:5].plot.bar();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\n\n'CCAvg' has strongest association with 'Personal Loan'","metadata":{}},{"cell_type":"markdown","source":"Filter columns with P-values less then 0.05 and store variables and it's coefficients into the dictionary","metadata":{}},{"cell_type":"code","source":"quant_df_main = {}\nfor i in log_mod_check.params[1:5].to_dict().keys():\n    if log_mod_check.pvalues[i] < 0.05:\n        quant_df_main[i] = log_mod_check.params[i]\n    else:\n        continue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quant_df_main","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quant_df_main_odds = {k : np.exp(v) for k, v in quant_df_main.items()}\nquant_df_main_odds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation :\n\n'Personal Loan' has statisticaly significant association with:\n\n    'Income' : coef = 0.03508\n    'CCAvg' : coef = 0.06879\nBoth variables are positively associated with 'Personal Loan'. As soon as both have one unit as $1000 we may say the following:\n\n    For each $1000 increase in 'Income' we expect the odds to sell Personal Loan to increase by 3.57%, holding everything else constant\n\n    For each $1000 increase in 'CCAvg' we expect the odds to sell Personal Loan to increase by 7.12%, holding everything else constant","metadata":{}},{"cell_type":"markdown","source":"###### CATEGORICAL VARIABLES\n\n'ZIP Code', 'Family', 'Education'\n\n'Family' and 'Education' are ordinal categorical variables so we may apply logistic regression direct to them. 'ZIP Code' is nominal, so we need to build dummy variables to check the association existence","metadata":{}},{"cell_type":"code","source":"cat_df = bank_per_loan_df[['ZIP Code', 'Family', 'Education', 'Personal Loan']].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'Family' and 'Education'","metadata":{}},{"cell_type":"code","source":"cat_df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_df.corr()['Personal Loan'][0:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_df.corr()['Personal Loan'][0:2].plot.bar();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Observation :\n\nFamily' and 'Education' has low association with 'Personal Loan'\n\n\n\n\nLet's check our confidence with logistic regresstion","metadata":{}},{"cell_type":"code","source":"cat_df['intercept'] = 1\nlog_mod_1 = sm.Logit(cat_df['Personal Loan'], cat_df[['intercept', 'Family', 'Education']]).fit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_mod_1.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation:\n\nWe can say with confidence that 'Family' and 'Education' both has statisticaly significant association with 'Personal Loan', since their p-value in logistic regression < 0.05","metadata":{}},{"cell_type":"markdown","source":"The bar chart of coefficient distribution","metadata":{}},{"cell_type":"code","source":"# exclude 'intercept'\nlog_mod_1.params[1:3].plot.bar();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation:\n\n'Education' has strongest association with 'Personal Loan'","metadata":{}},{"cell_type":"code","source":"cat_df_main = {}\nfor i in log_mod_1.params[1:3].to_dict().keys():\n    if log_mod_1.pvalues[i] < 0.05:\n        cat_df_main[i] = log_mod_1.params[i]\n    else:\n        continue \n    \ncat_df_main","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_df_odds_1 = {k : np.exp(v) for k, v in cat_df_main.items()}\ncat_df_odds_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation:\n\nConclusion:\n\n'Personal Loan' has statisticaly significant association with:\n\n    'Family' : coef = 0.16231\n    'Education' : coef = 0.54873\nBoth variables are positively associated with 'Personal Loan'. We may say the following:\n\n    For each unit increase in 'Family' we expect the odds to sell Personal Loan to increase by 17.62%, holding everything else constant\n\n    For each unit increase in 'Education' we expect the odds to sell Personal Loan to increase by 73.11%, holding everything else constant","metadata":{}},{"cell_type":"markdown","source":"##### ZIP Code","metadata":{}},{"cell_type":"code","source":"cat_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zip_df = cat_df[['Personal Loan', 'intercept','ZIP Code']].copy()\nzip_df.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets check how we can group the 'Zip Code' values to minimize the number of dummies","metadata":{}},{"cell_type":"code","source":"zip_df['ZIP Code'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make string version of original column, call it 'col'\nzip_df['ZIP Code_str'] = zip_df['ZIP Code'].astype(str)\nzip_df.info()\n\n# make the new columns using string indexing\nzip_df['zip_df_2'] = zip_df['ZIP Code_str'].str[0:2]\nzip_df['zip_df_3'] = zip_df['ZIP Code_str'].str[0:3]\nzip_df.info()\nzip_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zip_df['zip_df_3'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zip_df['zip_df_2'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zip_df['zip_df_2'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Guess this set is okay for the first view since we assume that the initial campaign of selling Personal Loans was evenly spreaded through all zip codes.","metadata":{}},{"cell_type":"markdown","source":"before we get dummies, let us drop ZIP Code, ZIP Code_str and zip_df_3","metadata":{}},{"cell_type":"code","source":"zip_2_df = copy.deepcopy(zip_df)\nzip_2_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zip_2_df.drop(['ZIP Code', 'ZIP Code_str','zip_df_3'], axis=1, inplace=True)\nzip_2_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's get dummies...","metadata":{}},{"cell_type":"code","source":"dum_zip_df = pd.get_dummies(zip_2_df, prefix = \"Z\", drop_first = True)\ndum_zip_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fit a logic model\n#exclude 'Personal Loan' from independ vars\ndum_zip_df_columns = dum_zip_df.columns.drop('Personal Loan').tolist()\ndum_zip_df_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_mod_2 = sm.Logit(dum_zip_df['Personal Loan'], dum_zip_df[dum_zip_df_columns]).fit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_mod_2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\n\nWe can say with confidence that any ZIP Code does not have statisticaly significant association with 'Personal Loan', since their p-value in logistic regression > 0.05","metadata":{}},{"cell_type":"markdown","source":"##### BINARY VARIABLES\n\n'Securities Account', 'CD Account', 'Online', 'Credit Card'","metadata":{}},{"cell_type":"code","source":"bin_df = bank_per_loan_df[['Personal Loan', 'Securities Account', 'CD Account', 'Online', 'CreditCard']].copy()\nbin_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bin_df.corr()['Personal Loan']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bin_df.corr()['Personal Loan'][1:].plot.bar();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\n\nCD Account' - the only one variable with moderate association","metadata":{}},{"cell_type":"code","source":"#Let's fit logistic regression\nbin_df['intercept'] = 1\nbin_df_colmn = bin_df.columns.drop('Personal Loan').tolist()\nlog_mod_3 = sm.Logit(bin_df['Personal Loan'], bin_df[bin_df_colmn]).fit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_mod_3.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_mod_4 = sm.Logit(bin_df['Personal Loan'], bin_df[['intercept', 'CD Account']]).fit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_mod_4.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bin_odds = {'CD Account' : np.exp(log_mod_4.params[1])}\nbin_odds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\n\n\nConclusion:\n\n'Personal Loan' has statisticaly significant positive association with only:\n\n    'CD Account' : coef = 2.40\nWe may say the following:\n\n    With customer been hold CD Account with The Bank we expect the odds to sell Personal Loan to increase 10 times, holding everything else constant","metadata":{}},{"cell_type":"markdown","source":"### Summary Conclusion:\n\n'Personal Loan' has statisticaly significant association with:\n\n    'CD Account' : coef = 2.40 : odds = 11.07\n    'Family' : coef = 0.16231 : odds = 1.176\n    'Education' : coef = 0.54873 : odds = 1.731\n    'Income' : coef = 0.03508 : odds = 1.036\n    'CCAvg' : coef = 0.06879 : odds = 1.071\nBoth variables are positively associated with 'Personal Loan'. We may say the following:\n\n    With customer been hold CD Account with The Bank we expect the odds to sell Personal Loan to increase 11 times, holding everything else constant\n\n    For each unit increase in 'Family' we expect the odds to sell Personal Loan to increase by 17.62%, holding everything else constant\n\n    For each unit increase in 'Education' we expect the odds to sell Personal Loan to increase by 73.11%, holding everything else constant\n\n    For each $1000 increase in 'Income' we expect the odds to sell Personal Loan to increase by 3.57%, holding everything else constant\n\n    For each $1000 increase in 'CCAvg' we expect the odds to sell Personal Loan to increase by 7.12%, holding everything else constant","metadata":{}},{"cell_type":"markdown","source":"###### ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Question 3: Get the target distribution.","metadata":{}},{"cell_type":"markdown","source":"## Target Label","metadata":{}},{"cell_type":"markdown","source":"Personal Loan will be the target","metadata":{}},{"cell_type":"code","source":"# 0 = didnt take loan in the last campaign (90.4%)\n# 1 = took loan in the last campaign (9.6%)\nbank_per_loan_df[\"Personal Loan\"].value_counts().to_frame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.value_counts(bank_per_loan_df[\"Personal Loan\"]).plot(kind=\"bar\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_no_sub = len(bank_per_loan_df[bank_per_loan_df['Personal Loan']==0])\nprint('count_no_sub :',count_no_sub)\ncount_sub = len(bank_per_loan_df[bank_per_loan_df['Personal Loan']==1])\nprint('count_sub :',count_sub)\npct_of_no_sub = count_no_sub/(count_no_sub+count_sub)\nprint('pct_of_no_sub')\nprint(\"percentage of no subscription is\", pct_of_no_sub*100)\npct_of_sub = count_sub/(count_no_sub+count_sub)\nprint(\"percentage of subscription\", pct_of_sub*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Looking into the distribution to the various attributes in relation with the target.","metadata":{}},{"cell_type":"code","source":"bank_per_loan_df.groupby(bank_per_loan_df['Personal Loan']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations: \n    \n1). The average Income of customers who took loan is more than double of the avg income of customers who didn’t take loan last year.\n\n2). The Avg. spending on credit cards per month ($000) is also more than double for the customer's who took loan.\n\n3). The average mortage for loan availing customers is approximately double for the not availing customers.\n\n4). Avg literacy is less for non loan takers.\n\nAs given in the data description that person who took loan in the last camping is 9.6%","metadata":{}},{"cell_type":"markdown","source":"###### ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Question 4: Split the data into training and test set in the ratio of 70:30 respectively.","metadata":{}},{"cell_type":"markdown","source":"# Data Split 70:30 Ratio","metadata":{}},{"cell_type":"code","source":"train_set, test_set = train_test_split(bank_per_loan_df.drop(['ID','Experience'], axis=1), test_size=0.3 , random_state= 77)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels = train_set.pop('Personal Loan')\ntest_labels = test_set.pop('Personal Loan')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set_indep = bank_per_loan_df.drop(['Experience' ,'ID'] , axis = 1).drop(labels= \"Personal Loan\" , axis = 1)\ntrain_set_dep = bank_per_loan_df[\"Personal Loan\"]\nX = np.array(train_set_indep)\nY = np.array(train_set_dep)\nX_Train = X[ :3500, :]\nX_Test = X[3501: , :]\nY_Train = Y[:3500, ]\nY_Test = Y[3501:, ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Also I have showing couple variation of models, and have also scaled the data to increase the accuracy of the model by standard scaler mthod, shown later below.","metadata":{}},{"cell_type":"markdown","source":"###### --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Question 5: Use different classification models (Logistic, K-NN and Naïve Bayes) to predict the likelihood of a liability customer buying personal loans","metadata":{}},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"logmodel = LogisticRegression()\nlogmodel.fit(X_Train,Y_Train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict = logmodel.predict(X_Test)\npredictProb = logmodel.predict_proba(X_Test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix\ncm = confusion_matrix(Y_Test, predict)\nclass_label = [\"Positive\", \"Negative\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report\nprint(classification_report(Y_Test, predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictProb = logmodel.predict_proba(X_Test)\nskplt.metrics.plot_cumulative_gain(Y_Test, predictProb)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skplt.metrics.plot_precision_recall(Y_Test, predictProb)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# K-NN","metadata":{}},{"cell_type":"markdown","source":"##### Cross Validation","metadata":{}},{"cell_type":"code","source":"# Creating odd list of K for KNN\nmyList = list(range(1,20))\n# Subsetting just the odd ones\nneighbors = list(filter(lambda x: x % 2 != 0, myList))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Empty list that will hold accuracy scores\nac_scores = []\n\n# Perform accuracy metrics for values from 1,3,5....19\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_Train, Y_Train)\n    \n    # Predict the response\n    Y_Pred = knn.predict(X_Test)\n    \n    # Evaluate accuracy\n    scores = accuracy_score(Y_Test, Y_Pred)\n    ac_scores.append(scores)\n\n# Changing to misclassification error\nMSE = [1 - x for x in ac_scores]\n\n# Determining best k\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Model","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors= 13 , weights = 'uniform', metric = 'euclidean')\nknn.fit(X_Train, Y_Train)    \npredicted = knn.predict(X_Test)\nacc = accuracy_score(Y_Test, predicted)\nprint(acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Misclassification Error vs K","metadata":{}},{"cell_type":"code","source":"plt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix\ncm1 = confusion_matrix(Y_Test, predicted)\nclass_label = [\"Positive\", \"Negative\"]\ndf_cm1 = pd.DataFrame(cm1, index = class_label, columns = class_label)\nsns.heatmap(df_cm1, annot = True, fmt = \"d\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report\nprint(classification_report(Y_Test, predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_probas17= knn.predict_proba(X_Test)\nskplt.metrics.plot_cumulative_gain(Y_Test, y_probas17)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skplt.metrics.plot_precision_recall(Y_Test, y_probas17)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Naive Bayes","metadata":{}},{"cell_type":"code","source":"# Model\nnaive_model = GaussianNB()\nnaive_model.fit(train_set, train_labels)\nprediction = naive_model.predict(test_set)\nnaive_model.score(test_set,test_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix\ncm2 = confusion_matrix(test_labels, prediction)\nclass_label = [\"Positive\", \"Negative\"]\ndf_cm2 = pd.DataFrame(cm2, index = class_label, columns = class_label)\nsns.heatmap(df_cm2, annot = True, fmt = \"d\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classififcation Report\nprint(classification_report(test_labels, prediction))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_probas67 = naive_model.predict_proba(X_Test)\nskplt.metrics.plot_cumulative_gain(Y_Test, y_probas67)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skplt.metrics.plot_precision_recall(Y_Test, y_probas67)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Comparison","metadata":{}},{"cell_type":"code","source":"models = []\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('NB', GaussianNB()))\n\n# Evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=12345)\n    cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n    \n# Boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# 6. Print the confusion matrix for all the above models","metadata":{}},{"cell_type":"code","source":"# Confusion Matrix\ncm = confusion_matrix(Y_Test, predict)\nclass_label = [\"Positive\", \"Negative\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\")\nplt.title(\"Confusion Matrix via Logistics Regression\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix\ncm1 = confusion_matrix(Y_Test, predicted)\nclass_label = [\"Positive\", \"Negative\"]\ndf_cm1 = pd.DataFrame(cm1, index = class_label, columns = class_label)\nsns.heatmap(df_cm1, annot = True, fmt = \"d\")\nplt.title(\"Confusion Matrix via K-NN\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix\ncm2 = confusion_matrix(test_labels, prediction)\nclass_label = [\"Positive\", \"Negative\"]\ndf_cm2 = pd.DataFrame(cm2, index = class_label, columns = class_label)\nsns.heatmap(df_cm2, annot = True, fmt = \"d\")\nplt.title(\"Confusion Matrix via Naive Bayes \")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Give your reasoning on which is the best model in this case and why it performs better?\n\n","metadata":{}},{"cell_type":"markdown","source":"### Model Comparison","metadata":{}},{"cell_type":"code","source":"models = []\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('NB', GaussianNB()))\n\n# Evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=12345)\n    cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n    \n# Boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nSummary\n\n    The aim of the Bank is to convert there liability customers into loan customers.\n    They want to set up a new marketing campaign; hence, they need information about the connection between the variables given in the data.\n    Three classification algorithms were used in this study.\n    From the above graph , it seems like 'Logistic Regression' algorithm have the highest accuracy and we can choose that as our final model\n    \n    \n    The logistic Regression model is the best as the accuracy of the train and test set is almost similar and also the precsion and recall accuracy is good. The confusion matrix is also better in comparision to other models.\n\nThe requirement is to classify the target. The KNN is distance based which not perfect for this situation.Though the accuracy is good but confusion matrix tells that is correct predictions is not that much acceptable.\n\nThe Naive Bayes giving the ccuracy less in comaprision to other models meaning the probability of determing the target correctly is less.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"##### ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"#### Another apporach for model, cleaning the outlier in the dataset for Zscore greater than 3, scaling the data via standardization, ROC , others","metadata":{}},{"cell_type":"code","source":"bank_per_loan_1_df = copy.deepcopy(bank_per_loan_df)\nbank_per_loan_1_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Due to these outliers’ bulk of the data in the Mortgage is at the left and the right tail is longer. This is called right skewness. One way to remove the skewness is by doing the z-score.","metadata":{}},{"cell_type":"code","source":"bank_per_loan_1_df['Mortgage_zscore'] = np.abs(stats.zscore(bank_per_loan_1_df['Mortgage']))\nbank_per_loan_1_df = bank_per_loan_1_df[bank_per_loan_1_df['Mortgage_zscore']<3]\nbank_per_loan_1_df.drop('Mortgage_zscore', axis =1, inplace =True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_per_loan_1_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removed the outlier\nHere I had chosen those rows only whose z_score is less than 3, it can vary accordingly. Here we had dropped more than 100+ rows which contain outliers and now we can start with the model building","metadata":{}},{"cell_type":"code","source":"bank_per_loan_1_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, no need for ‘ID’, ‘ZIP_Code’ & ‘Experience’ columns for further analysis since ‘ID’ and ‘ZIP_Code’ are just numbers of series & ‘Experience’ is highly correlated with ‘Age’.","metadata":{}},{"cell_type":"code","source":"#also droping - 'ID','Experience'\nbank_per_loan_1_df.drop('ID', axis =1, inplace =True)\nbank_per_loan_1_df.drop('Experience', axis =1, inplace =True)\nbank_per_loan_1_df.drop('ZIP Code', axis =1, inplace =True)\nbank_per_loan_1_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_per_loan_1_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"X_1 = bank_per_loan_1_df.drop('Personal Loan', axis =1) \ny_1 = bank_per_loan_1_df['Personal Loan']\n\nX1_train, X1_test, y1_train, y1_test = train_test_split(X_1,y_1,test_size=0.3, random_state = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LogReg_model = LogisticRegression()\nLogReg_model.fit(X1_train,y1_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_1_pred = LogReg_model.predict(X1_test)\nprint(classification_report(y1_test,y_1_pred))\nprint(accuracy_score(y1_test,y_1_pred))\nprint(confusion_matrix(y1_test,y_1_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LogReg_prob = LogReg_model.predict_proba(X1_test)\nfpr1,tpr1, thresholds1 = roc_curve(y1_test, LogReg_prob[:,1])\nroc_auc1 = auc(fpr1,tpr1)\nprint(\"Area under the ROC curve  :  %f\" %roc_auc1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### STANDARDIZATION","metadata":{}},{"cell_type":"code","source":"col_names_standard = bank_per_loan_1_df.columns\nscaler_1 = preprocessing.StandardScaler()\nscaled_X1_train = scaler_1.fit_transform(X1_train)\nscaled_X1_test = scaler_1.fit_transform(X1_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LogReg_scaled_model = LogisticRegression()\nLogReg_scaled_model.fit(scaled_X1_train,y1_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_1_scaled_pred = LogReg_scaled_model.predict(scaled_X1_test)\nprint(classification_report(y1_test,y_1_scaled_pred))\nprint(accuracy_score(y1_test,y_1_scaled_pred))\nprint(confusion_matrix(y1_test,y_1_scaled_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LogReg_scaled_prob = LogReg_scaled_model.predict_proba(scaled_X1_test)\nfpr2,tpr2, thresholds2 = roc_curve(y1_test, LogReg_scaled_prob[:,1])\nroc_auc2 = auc(fpr2,tpr2)\nprint(\"Area under the ROC curve  :  %f\" % roc_auc2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get an increase in accuracy and clearly see the difference between evaluation metrics with standardization of the data. As mentioned before, accuracy alone can’t define my model how well it predicted so we will play with recall now.","metadata":{}},{"cell_type":"markdown","source":"We get a recall value of 66%, which means our model did much better in predicting True Positives.\n\nAlso, the area under the curve is around 96%, much higher than previously.\n\nFurther, we will analyze other models with only scaled data.","metadata":{}},{"cell_type":"markdown","source":"# Naive Bayes","metadata":{}},{"cell_type":"code","source":"naive_model_2 = GaussianNB()\nnaive_model_2.fit(scaled_X1_train, y1_train)\n\ny_2_scaled_pred = naive_model_2.predict(scaled_X1_test)\nprint(classification_report(y1_test,y_2_scaled_pred))\nprint(accuracy_score(y1_test,y_2_scaled_pred))\nprint(confusion_matrix(y1_test,y_2_scaled_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"naive_scaled_prob = naive_model_2.predict_proba(scaled_X1_test)\nfpr3,tpr3, thresholds3 = roc_curve(y1_test, naive_scaled_prob[:,1])\nroc_auc3 = auc(fpr3,tpr3)\nprint(\"Area under the ROC curve  :  %f\" % roc_auc3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We got an accuracy score of around 90% with a recall value of 64% which is much less as compared to the Logistic Regression.\n\nAlso, the area under the curve is around 93%, less than the logistic regression one.\n\nHence Naive Bayes terms out to be not a good classifier with this particular dataset.","metadata":{}},{"cell_type":"markdown","source":"# kNN","metadata":{}},{"cell_type":"code","source":"kNN_scaled_model = KNeighborsClassifier(n_neighbors= 3)\nkNN_scaled_model.fit(scaled_X1_train, y1_train)  \n\ny_3_scaled_pred = kNN_scaled_model.predict(scaled_X1_test)\nprint(classification_report(y1_test,y_3_scaled_pred))\nprint(accuracy_score(y1_test,y_3_scaled_pred))\nprint(confusion_matrix(y1_test,y_3_scaled_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kNN_scaled_prob = kNN_scaled_model.predict_proba(scaled_X1_test)\nfpr4,tpr4, thresholds4 = roc_curve(y1_test, kNN_scaled_prob[:,1])\nroc_auc4 = auc(fpr4,tpr4)\nprint(\"Area under the ROC curve  :  %f\" % roc_auc4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And here we are with around 97% accuracy in determining if a customer will buy the personal loan or not. Also, the recall value is 75% is much better than logistic regression and Naive Bayes algorithms. Also, the area under the curve is fairly good.","metadata":{}},{"cell_type":"markdown","source":"# SVM (Support Vector Machine)","metadata":{}},{"cell_type":"code","source":"clf_1 = svm.SVC(C=3, kernel ='rbf', probability = True)\nclf_1.fit(scaled_X1_train, y1_train)\n\ny_4_scaled_pred = clf_1.predict(scaled_X1_test)\nprint(classification_report(y1_test,y_4_scaled_pred))\nprint(accuracy_score(y1_test,y_4_scaled_pred))\nprint(confusion_matrix(y1_test,y_4_scaled_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_scaled_prob = clf_1.predict_proba(scaled_X1_test)\nfpr5,tpr5, thresholds5 = roc_curve(y1_test, svm_scaled_prob[:,1])\nroc_auc5 = auc(fpr5,tpr5)\nprint(\"Area under the ROC curve  :  %f\" % roc_auc5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We got 98% accuracy score with 81% recall value, also the area under the curve is about 99%… JUST WOW…!","metadata":{}},{"cell_type":"markdown","source":"##### -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"## Another  approach, by breaking down the features into sub category,  then doing the model analysis","metadata":{}},{"cell_type":"code","source":"loan_ml1_df = copy.deepcopy(bank_per_loan_df)\nloan_ml1_df = loan_ml1_df.drop('Experience', axis=1)\nloan_ml1_df = loan_ml1_df.drop('ID', axis=1)\nloan_ml1_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_bin = 20\nforce_bin = 3\n\n# define a binning function\ndef mono_bin(Y, X, n = max_bin):\n    \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]\n    r = 0\n    while np.abs(r) < 1:\n        try:\n            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n            d2 = d1.groupby('Bucket', as_index=True)\n            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n            n = n - 1 \n        except Exception as e:\n            n = n - 1\n\n    if len(d2) == 1:\n        n = force_bin         \n        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n        if len(np.unique(bins)) == 2:\n            bins = np.insert(bins, 0, 1)\n            bins[1] = bins[1]-(bins[1]/2)\n        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n        d2 = d1.groupby('Bucket', as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"MIN_VALUE\"] = d2.min().X\n    d3[\"MAX_VALUE\"] = d2.max().X\n    d3[\"COUNT\"] = d2.count().Y\n    d3[\"EVENT\"] = d2.sum().Y\n    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n    d3=d3.reset_index(drop=True)\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    \n    return(d3)\n\ndef char_bin(Y, X):\n        \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]    \n    df2 = notmiss.groupby('X',as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"COUNT\"] = df2.count().Y\n    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n    d3[\"EVENT\"] = df2.sum().Y\n    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    d3 = d3.reset_index(drop=True)\n    \n    return(d3)\n\ndef data_vars(df1, target):\n    \n    stack = traceback.extract_stack()\n    filename, lineno, function_name, code = stack[-2]\n    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n    \n    x = df1.dtypes.index\n    count = -1\n    \n    for i in x:\n        if i.upper() not in (final.upper()):\n            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n                conv = mono_bin(target, df1[i])\n                conv[\"VAR_NAME\"] = i\n                count = count + 1\n            else:\n                conv = char_bin(target, df1[i])\n                conv[\"VAR_NAME\"] = i            \n                count = count + 1\n                \n            if count == 0:\n                iv_df = conv\n            else:\n                iv_df = iv_df.append(conv,ignore_index=True)\n    \n    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n    iv = iv.reset_index()\n    return(iv_df,iv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_iv, IV = data_vars(loan_ml1_df,loan_ml1_df['Personal Loan'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_iv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IV.sort_values('IV')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_ml1_df['California']=(loan_ml1_df['ZIP Code']<96200).astype(int)\nloan_ml1_df['undergraduate']=(loan_ml1_df['Education']==1).astype(int)\nloan_ml1_df['graduate']=(loan_ml1_df['Education']==2).astype(int)\nloan_ml1_df['family_1']=(loan_ml1_df['Family']==1).astype(int)\nloan_ml1_df['family_2']=(loan_ml1_df['Family']==2).astype(int)\nloan_ml1_df['family_3']=(loan_ml1_df['Family']==3).astype(int)\nloan_ml1_df=loan_ml1_df.drop('ZIP Code',axis=1)\nloan_ml1_df=loan_ml1_df.drop('Education',axis=1)\nloan_ml1_df=loan_ml1_df.drop('Family',axis=1)\nloan_ml1_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_ml1_df['Age_0_25']=(loan_ml1_df['Age']<=25).astype(int)\nloan_ml1_df['Age_25_30']=(loan_ml1_df['Age']>25).astype(int) & (loan_ml1_df['Age']<=30).astype(int)\nloan_ml1_df['Age_30_35']=(loan_ml1_df['Age']>30).astype(int) & (loan_ml1_df['Age']<=35).astype(int)\nloan_ml1_df['Age_35_40']=(loan_ml1_df['Age']>35).astype(int) & (loan_ml1_df['Age']<=40).astype(int)\nloan_ml1_df['Age_40_45']=(loan_ml1_df['Age']>40).astype(int) & (loan_ml1_df['Age']<=45).astype(int)\nloan_ml1_df['Age_45_50']=(loan_ml1_df['Age']>45).astype(int) & (loan_ml1_df['Age']<=50).astype(int)\nloan_ml1_df['Age_50_55']=(loan_ml1_df['Age']>50).astype(int) & (loan_ml1_df['Age']<=55).astype(int)\nloan_ml1_df['Age_55_60']=(loan_ml1_df['Age']>55).astype(int) & (loan_ml1_df['Age']<=60).astype(int)\nloan_ml1_df['Age_60_65']=(loan_ml1_df['Age']>60).astype(int) & (loan_ml1_df['Age']<=65).astype(int)\nloan_ml1_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_ml1_df['CC_0_1']=(loan_ml1_df['CCAvg']<=1).astype(int)\nloan_ml1_df['CC_1_2']=(loan_ml1_df['CCAvg']>1).astype(int) & (loan_ml1_df['CCAvg']<=2).astype(int)\nloan_ml1_df['CC_2_3']=(loan_ml1_df['CCAvg']>2).astype(int) & (loan_ml1_df['CCAvg']<=3).astype(int)\nloan_ml1_df['CC_3_4']=(loan_ml1_df['CCAvg']>3).astype(int) & (loan_ml1_df['CCAvg']<=4).astype(int)\nloan_ml1_df['CC_4_5']=(loan_ml1_df['CCAvg']>4).astype(int) & (loan_ml1_df['CCAvg']<=5).astype(int)\nloan_ml1_df['CC_5_6']=(loan_ml1_df['CCAvg']>5).astype(int) & (loan_ml1_df['CCAvg']<=6).astype(int)\nloan_ml1_df['CC_6_7']=(loan_ml1_df['CCAvg']>6).astype(int) & (loan_ml1_df['CCAvg']<=7).astype(int)\nloan_ml1_df['CC_7_8']=(loan_ml1_df['CCAvg']>7).astype(int) & (loan_ml1_df['CCAvg']<=8).astype(int)\nloan_ml1_df['CC_8_9']=(loan_ml1_df['CCAvg']>8).astype(int) & (loan_ml1_df['CCAvg']<=9).astype(int)\nloan_ml1_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_ml1_df['Income_0_20']=(loan_ml1_df['Income']<=20).astype(int)\nloan_ml1_df['Income_20_40']=(loan_ml1_df['Income']>20).astype(int) & (loan_ml1_df['Income']<=40).astype(int)\nloan_ml1_df['Income_40_60']=(loan_ml1_df['Income']>40).astype(int) & (loan_ml1_df['Income']<=60).astype(int)\nloan_ml1_df['Income_60_80']=(loan_ml1_df['Income']>60).astype(int) & (loan_ml1_df['Income']<=80).astype(int)\nloan_ml1_df['Income_80_100']=(loan_ml1_df['Income']>80).astype(int) & (loan_ml1_df['Income']<=100).astype(int)\nloan_ml1_df['Income_100_120']=(loan_ml1_df['Income']>100).astype(int) & (loan_ml1_df['Income']<=120).astype(int)\nloan_ml1_df['Income_120_140']=(loan_ml1_df['Income']>120).astype(int) & (loan_ml1_df['Income']<=140).astype(int)\nloan_ml1_df['Income_140_160']=(loan_ml1_df['Income']>140).astype(int) & (loan_ml1_df['Income']<=160).astype(int)\nloan_ml1_df['Income_160_180']=(loan_ml1_df['Income']>160).astype(int) & (loan_ml1_df['Income']<=180).astype(int)\nloan_ml1_df['Income_180_200']=(loan_ml1_df['Income']>180).astype(int) & (loan_ml1_df['Income']<=200).astype(int)\nloan_ml1_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_ml1_df['Mortgage_0_75']=(loan_ml1_df['Mortgage']==0).astype(int)\nloan_ml1_df['Mortgage_75_125']=(loan_ml1_df['Mortgage']>=75).astype(int) & (loan_ml1_df['Mortgage']<125).astype(int)\nloan_ml1_df['Mortgage_125_175']=(loan_ml1_df['Mortgage']>=125).astype(int) & (loan_ml1_df['Mortgage']<175).astype(int)\nloan_ml1_df['Mortgage_175_225']=(loan_ml1_df['Mortgage']>=175).astype(int) & (loan_ml1_df['Mortgage']<225).astype(int)\nloan_ml1_df['Mortgage_225_275']=(loan_ml1_df['Mortgage']>=225).astype(int) & (loan_ml1_df['Mortgage']<275).astype(int)\nloan_ml1_df['Mortgage_275_325']=(loan_ml1_df['Mortgage']>=275).astype(int) & (loan_ml1_df['Mortgage']<325).astype(int)\nloan_ml1_df['Mortgage_325_400']=(loan_ml1_df['Mortgage']>=325).astype(int) & (loan_ml1_df['Mortgage']<400).astype(int)\nloan_ml1_df['Mortgage_400_500']=(loan_ml1_df['Mortgage']>=400).astype(int) & (loan_ml1_df['Mortgage']<500).astype(int)\nloan_ml1_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_ml1_df[['Age_sq','Income_sq','CCAvg_sq','Mortgage_sq']]=loan_ml1_df[['Age','Income','CCAvg','Mortgage']].apply(lambda x: np.square(x))\nloan_ml1_df[['Age_sqrt','Income_sqrt','CCAvg_sqrt','Mortgage_sqrt']]=loan_ml1_df[['Age','Income','CCAvg','Mortgage']].apply(lambda x: np.sqrt(x))\nloan_ml1_df[['Age_ln','Income_ln','CCAvg_ln','Mortgage_ln']]=loan_ml1_df[['Age','Income','CCAvg','Mortgage']].apply(lambda x: np.log(x))\nloan_ml1_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_ml1_df.loc[loan_ml1_df['Mortgage_ln']<0,['Mortgage_ln']]= 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correl_13 = loan_ml1_df.corr().abs()\n# Select upper triangle of correlation matrix\nupper = correl_13.where(np.triu(np.ones(correl_13.shape), k=1).astype(np.bool))\n# Find index of feature columns with correlation greater than 0.75\nto_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\nto_drop\ncorrel_13.to_csv('file13.csv',index=False)\ncorrel_13.style.background_gradient(cmap='coolwarm').set_precision(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# can remove age and its transformations\n# can remove mortgage and its transformations except mortgage_sq\n# can remove income transformations\n# can remove all CCavg variables\nloan_ml1_df.columns\nloan_ml1_df_new=loan_ml1_df.drop(['Age','CCAvg','Income','Mortgage','Age_sq', 'Income_sq', 'CCAvg_sq',\n                'Age_sqrt','Income_sqrt', 'CCAvg_sqrt', 'Mortgage_sqrt', 'Age_ln','Income_ln',\n                'CCAvg_ln', 'Mortgage_ln','Age_0_25', 'Age_25_30', 'Age_30_35','Age_35_40', \n                'Age_40_45', 'Age_45_50', 'Age_50_55', 'Age_55_60','Age_60_65','California',\n                'Mortgage_sq'],axis=1)\nloan_ml1_df_new\n# no dependency on online, creditcard, securities account\n# Customers having high CCAvg need personal loan\n# Family with income less than 100k are less likely to take loan\n# higher mortgage is more likely to get the loan\n# Income with more than 50 is more likely to get the personal loan\n# customers having COD account have high prob of taking loan\n# undergraduate has very less prob of taking the loan\n# family size of more than 3 are more likely to get the loan\n# 'CC_0_1', 'CC_1_2', 'CC_2_3', 'CC_3_4', 'CC_4_5', 'CC_5_6','CC_6_7', 'CC_7_8', 'CC_8_9',\n# 'Mortgage_0_75', 'Mortgage_75_125', 'Mortgage_125_175','Mortgage_175_225','Mortgage_225_275', 'Mortgage_275_325', 'Mortgage_325_400', 'Mortgage_400_500',","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_ml1_df_new.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correl_24 = loan_ml1_df_new.corr().abs()\n# Select upper triangle of correlation matrix\nupper = correl_24.where(np.triu(np.ones(correl_24.shape), k=1).astype(np.bool))\n# Find index of feature columns with correlation greater than 0.75\nto_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\nto_drop\ncorrel_24.to_csv('file24.csv',index=False)\ncorrel_24.style.background_gradient(cmap='coolwarm').set_precision(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X7=loan_ml1_df_new.iloc[:,1:]\ny7=loan_ml1_df_new.iloc[:,0]\nX7_train,X7_test,y7_train,y7_test=train_test_split(X7,y7,test_size=0.3,random_state=28)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"X_train shape   \",X7_train.shape)\nprint(\"X_test shape   \",X7_test.shape)\nprint(\"y_train shape   \",y7_train.shape)\nprint(\"y_test shape   \",y7_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logit_model_base7=sm.Logit(endog=y7_train,exog=X7_train)\nresult_17=logit_model_base7.fit()\nprint(result_17.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def elimination(x,sl,y):\n    numvars=len(x.columns)\n    for i in range(0,numvars):\n        lr=sm.Logit(y,x.values).fit()\n        maxvar=max(lr.pvalues)\n        if maxvar>sl:\n            for j in range(0,numvars-i):\n                if(lr.pvalues[j]==maxvar):\n                    del x[x.columns[j]]\n    lr.summary()\n    return x\n\nsl = 0.05\nx7_model=elimination(X7_train,sl,y7_train)\nlr7=sm.Logit(endog=y7_train,exog=x7_model).fit()\nprint(lr7.summary())\n\nvif=pd.DataFrame()\nvif['VIF Factor']=[variance_inflation_factor(x7_model.values,i) for i in range(x7_model.shape[1])]\nvif['features']=x7_model.columns\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del x7_model['CC_0_1']\nvif=pd.DataFrame()\nvif['VIF Factor']=[variance_inflation_factor(x7_model.values,i) for i in range(x7_model.shape[1])]\nvif['features']=x7_model.columns\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del x7_model['Online']\nvif=pd.DataFrame()\nvif['VIF Factor']=[variance_inflation_factor(x7_model.values,i) for i in range(x7_model.shape[1])]\nvif['features']=x7_model.columns\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del x7_model['CreditCard']\nvif=pd.DataFrame()\nvif['VIF Factor']=[variance_inflation_factor(x7_model.values,i) for i in range(x7_model.shape[1])]\nvif['features']=x7_model.columns\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del x7_model['CC_8_9']\nvif=pd.DataFrame()\nvif['VIF Factor']=[variance_inflation_factor(x7_model.values,i) for i in range(x7_model.shape[1])]\nvif['features']=x7_model.columns\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del x7_model['CC_1_2']\ndel x7_model['CC_2_3']\ndel x7_model['CC_7_8']\nvif=pd.DataFrame()\nvif['VIF Factor']=[variance_inflation_factor(x7_model.values,i) for i in range(x7_model.shape[1])]\nvif['features']=x7_model.columns\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del x7_model['CC_6_7']\ndel x7_model['Income_80_100']\nvif=pd.DataFrame()\nvif['VIF Factor']=[variance_inflation_factor(x7_model.values,i) for i in range(x7_model.shape[1])]\nvif['features']=x7_model.columns\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mylist7=list(x7_model.columns)\nprint(mylist7)\nprint(X7_test)\nprint(X7_train)\nX7_test=X7_test.loc[:, X7_test.columns.str.contains('|'.join(mylist7))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X7_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X7_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X7","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x7_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"lr7 = LogisticRegression(C=1.0, class_weight=None,    dual=False, fit_intercept=True,    intercept_scaling=1, max_iter=100,    multi_class='ovr', n_jobs=1, penalty='l2',    random_state=42, solver='liblinear',    tol=0.0001, verbose=0, warm_start=False)\nprint(lr7.fit(X7_train, y7_train))\nprint(lr7.score(X7_test, y7_test))\nprint(lr7.predict(X7_test.iloc[[0]]))\nprint(lr7.predict_proba(X7_test.iloc[[0]]))\nprint(lr7.predict_log_proba(X7_test.iloc[[0]]))\nprint(lr7.decision_function(X7_test.iloc[[0]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"intercept is the log odds of the baseline condition. We can convert it back to a percent accuracy (proportion)","metadata":{}},{"cell_type":"code","source":"lr7.intercept_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Using the inverse logit function, we see that the baseline for personal loan approval is 3.11%:","metadata":{}},{"cell_type":"code","source":"def inv_logit(p):\n    return np.exp(p) / (1 + np.exp(p))\ninv_logit(lr7.intercept_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred7 = lr7.predict(X7_test)\nskplt.metrics.plot_confusion_matrix(y7_test, y_pred7)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_probas17 = lr7.predict_proba(X7_test)\nskplt.metrics.plot_cumulative_gain(y7_test, y_probas17)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skplt.metrics.plot_precision_recall(y7_test, y_probas17)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 4))\nfi_viz = FeatureImportances(lr7)\nfi_viz.fit(X7, y7)\nfi_viz.poof()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Naive Bayes","metadata":{}},{"cell_type":"code","source":"nb7 = GaussianNB(priors=None, var_smoothing=1e-09)\nprint(nb7.fit(X7_train, y7_train))\nprint(nb7.score(X7_test, y7_test))\nprint(nb7.predict(X7_test.iloc[[0]]))\nprint(nb7.predict_proba(X7_test.iloc[[0]]))\nprint(nb7.predict_log_proba(X7_test.iloc[[0]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred27 = nb7.predict(X7_test)\nskplt.metrics.plot_confusion_matrix(y7_test, y_pred27)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_probas27 = nb7.predict_proba(X7_test)\nskplt.metrics.plot_cumulative_gain(y7_test, y_probas27)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skplt.metrics.plot_precision_recall(y7_test, y_probas27)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KNN","metadata":{}},{"cell_type":"code","source":"knc7 = KNeighborsClassifier(algorithm='auto',  leaf_size=30, metric='minkowski',  metric_params=None, n_jobs=1, n_neighbors=5,  p=2, weights='uniform')\nprint(knc7.fit(X7_train, y7_train))\nprint(knc7.score(X7_test, y7_test))\nprint(knc7.predict(X7_test.iloc[[0]]))\nprint(knc7.predict_proba(X7_test.iloc[[0]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred37 = knc7.predict(X7_test)\nskplt.metrics.plot_confusion_matrix(y7_test, y_pred37)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_probas37= knc7.predict_proba(X7_test)\nskplt.metrics.plot_cumulative_gain(y7_test, y_probas37)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skplt.metrics.plot_precision_recall(y7_test, y_probas37)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Additional EDA Analysis ","metadata":{}},{"cell_type":"markdown","source":"##### What are those Main Characteristics that has a higher association with Loan Fact and what the strength of association ?\n\nHere is a subset of the initial data frame with just characteristics that have a positive association with 'Personal Loan' and the size of association is higher than moderate","metadata":{}},{"cell_type":"code","source":"exp_df = bank_per_loan_df[['Income', 'CCAvg', 'Family', 'Education', 'CD Account', 'Personal Loan']].copy()\nexp_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's apply logistic regression on this subset","metadata":{}},{"cell_type":"code","source":"exp_df['intercept'] = 1\n\nlog_mod_5 = sm.Logit(exp_df['Personal Loan'], exp_df[['intercept','Income', 'CCAvg', 'Family', 'Education', 'CD Account']]).fit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get P-Values for each variable","metadata":{}},{"cell_type":"code","source":"log_mod_5.pvalues[1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### All p-values are less than 0.05","metadata":{}},{"cell_type":"markdown","source":"Get Odds for each variable","metadata":{}},{"cell_type":"code","source":"odds_exp = np.exp(log_mod_5.params)\nodds_exp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"odds_df = pd.DataFrame(odds_exp[1:], columns = [\"Odds\"])\nodds_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"odds_df['odds_increment'] = odds_df.Odds\nodds_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is the data frame with Main Characteristics ...\n\n... and their odds to increase the chance to sell Personal Loan with increase value of variable by one unit","metadata":{}},{"cell_type":"code","source":"odds_df.sort_values('Odds', ascending = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### The chart demonstrating the proportion of strength of association between Personal Loan and values of Main Characteristic","metadata":{}},{"cell_type":"code","source":"sizes = odds_df.Odds.tolist()# list of sizes of slices\nlabels = odds_df.index.tolist() # list of labels \nexplode = (0.15, 0.1, 0.2, 0.1, 0)  # \"explode\" the 2nd and 3rd slices  \nfig = plt.figure(figsize=(10, 5))\nplt.suptitle('The Proportion of Strength of Association  Between  \\n Personal Loan and Main Characteristics', \\\n          fontsize = 14, y = 1.18)\nplt.axis('equal'); # set aspect ration as equal to make sure the pie is drawn as a circle\nplt.pie(sizes, labels = labels, explode = explode, radius = 1.5, \\\n        shadow = True, startangle = 90,autopct= '%1.1f%%')\n\nplt.savefig('proportion_of_stregth_of_association1.png', bbox_inches = 'tight');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### What the Segments of Main Characteristics, that has a higher strength of association with Personal Loan?","metadata":{}},{"cell_type":"markdown","source":"Lets get a closer look at each of Main Characteristics","metadata":{}},{"cell_type":"markdown","source":"#### CD Account\n\n\nHere is the distribution of \"Personal Loan\" values among groups of \"CD Account\" values","metadata":{}},{"cell_type":"code","source":"series_cd = exp_df[exp_df['Personal Loan'] == 1]['CD Account'].value_counts()\nseries_cd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_cdd = exp_df[exp_df['Personal Loan'] == 0]['CD Account'].value_counts()\nseries_cdd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(dict( NO_PL= series_cdd, PL= series_cd,)).plot.bar(figsize = (8,6))\nplt.ylabel('Frequency')\nplt.xticks(np.arange(2),('No CD Account','CD Account'), rotation = 'horizontal')\nplt.legend(('NO Personal Loan', 'Personal Loan'));\nplt.title('Distribution of \"Personal Loan\" Values \\n among Groups of \"CD Account\" Values', fontsize = 14, y = 1.05);\nplt.savefig('distribution_of_PL_among_CDacc1.png', bbox_inches = 'tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\nWe may say that the proportion of persons who has Personal Loan among them who has CD account with The Bank is quit high.\n\n\nLet's see the exact number of proportion of \"loanees\" among \"depositees\"","metadata":{}},{"cell_type":"code","source":"series = exp_df[exp_df['CD Account'] == 1]['Personal Loan'].value_counts()\nseries","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.axis('equal')\nplt.title('Proportion of Customers Who Have Personal Loan and Who Don\\'t,\\n among CD Account Holders', \\\n          fontsize = 14, y = 1.2)\nlabels = ['NO Personal Loan','Personal Loan']\nplt.pie(series, labels = labels,autopct= '%1.1f%%', shadow = True,explode = (0.1, 0), radius = 1.6, startangle = 90)\nplt.savefig('Proportion_of_loanees_among_depositees1.png', bbox_inches = 'tight');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Conclusion\n\n    46.4% of CD Account Holders have Perconal Loan.\n    For 'CD Account' characteristic - the main segment to sell Personal Loan is the people who already have a CD Account with the Bank.\n    Target value of 'CD Account' variable = 1","metadata":{}},{"cell_type":"markdown","source":"#### Education\n\n\nHere is the distribution of \"Personal Loan\" values among groups of \"Education\" values","metadata":{}},{"cell_type":"code","source":"series_ed = exp_df[exp_df['Personal Loan'] == 1]['Education'].value_counts()\nseries_ed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_edd = exp_df[exp_df['Personal Loan'] == 0]['Education'].value_counts()\nseries_edd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(dict(NO_PL= series_edd, PL= series_ed)).plot.bar(figsize = (8,6))\nplt.ylabel('Frequency')\nplt.xlabel('Education Level')\nplt.xticks(np.arange(3),('1','2','3'), rotation = 'horizontal')\nplt.legend(('NO Personal Loan', 'Personal Loan'))\nplt.title('Distribution of \"Personal Loan\" Values \\n among Groups of \"Education\" Values', fontsize = 14, y = 1.05);\nplt.savefig('distribution_PL_among_Education1.png', bbox_inches = 'tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Observations \n\nWe may say that the proportion of persons who has Personal Loan among them who has Third and Second Level of Education is higher than proportion among people who has First level of Edication.\n\nLet's see the exact numbers of proportions.","metadata":{}},{"cell_type":"code","source":"series_edu_3 = exp_df[exp_df['Education'] == 3]['Personal Loan'].value_counts()\nseries_edu_3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_edu_2 = exp_df[exp_df['Education'] == 2]['Personal Loan'].value_counts()\nseries_edu_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_edu_1 = exp_df[exp_df['Education'] == 1]['Personal Loan'].value_counts()\nseries_edu_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = ['NO Personal Loan','Personal Loan']\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (18,6),subplot_kw=dict(aspect=\"equal\"))\nplt.axis('equal')\nax1.pie(series_edu_3, labels = labels, autopct= '%1.1f%%', shadow = True,explode = (0, 0.1), radius = 1.25, startangle = 90)\nax1.set_title('Education Level 3',fontsize = 14, y = 1.1)\n\nax2.pie(series_edu_2, labels = labels, autopct= '%1.1f%%', shadow = True,explode = (0, 0.1), radius = 1.25, startangle = 90)\nax2.set_title('Education Level 2', fontsize = 14, y = 1.1)\n\nax3.pie(series_edu_1, labels = labels, autopct= '%1.1f%%', shadow = True,explode = (0, 0.1), radius = 1.25, startangle = 90);\nax3.set_title('Education Level 1',fontsize = 14, y = 1.1)\n\nplt.suptitle('Proportion of Customers Who Have Personal Loan and Who Don\\'t, among CD Account Holders', \\\n             fontsize = 16, y = 1.12);\n\nplt.savefig('Proportion_of_PL_among edu_levels1.png', bbox_inches = 'tight');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_edu_4 = exp_df[exp_df['Personal Loan'] == 1]['Education'].value_counts()\nseries_edu_4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.axis('equal')\nplt.title('Proportion of Customers With Different Levels of Education \\n among Personal Loan Holders', \\\n          fontsize = 14, y = 1.3)\nlabels = ['Education Level  3',' Education Level 2','Education Level 1']\nplt.pie(series_edu_4, labels = labels, autopct= '%1.2f%%', shadow = True,explode = (0.1, 0, 0), radius = 1.6, startangle = 90);\nplt.savefig('Proportion_edu_levels_among_PL1.png', bbox_inches = 'tight');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Conclusion\n\n\n    42.7% and 37.9% of persons who have Personal Loan, have Education level 3 and Level 2 respectively.\n    For 'Education' characteristic - the main segments to sell Personal Loan is the people who have Second and Third levels of education\n    Target values of 'Education' variable are 3 and 2 in descending order of priority","metadata":{}},{"cell_type":"markdown","source":"#### Family\n\nHere is the distribution of \"Personal Loan\" values among groups of \"Family\" valuesx","metadata":{}},{"cell_type":"code","source":"series_fam = exp_df[exp_df['Personal Loan'] == 1]['Family'].value_counts()\nseries_fam","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_famm = exp_df[exp_df['Personal Loan'] == 0]['Family'].value_counts()\nseries_famm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(dict( NO_PL = series_famm, PL= series_fam,)).plot.bar(figsize = (8,6))\nplt.ylabel('Frequency')\nplt.xlabel('Family Size')\nplt.xticks(np.arange(4),('1', '2', '3', '4'), rotation = 'horizontal')\nplt.legend(('NO Personal Loan', 'Personal Loan'));\nplt.title('Distribution of \"Personal Loan\" Values \\n among Groups of \"Family\" Values', fontsize = 14, y = 1.05);\nplt.savefig('distribution_of_PL_among_family1.png', bbox_inches = 'tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\n\nWe may say that the proportion of persons who has Personal Loan among them who has Family size 2 and 3 is highest proportion. \n\n\nLet's see the exact number of that proportions of \"loanees\" among \"depositees\"","metadata":{}},{"cell_type":"code","source":"series_fam_3 = exp_df[exp_df['Family'] == 3]['Personal Loan'].value_counts()\nseries_fam_3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_fam_4 = exp_df[exp_df['Family'] == 4]['Personal Loan'].value_counts()\nseries_fam_4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = ['NO Personal Loan','Personal Loan']\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize = (12,6),subplot_kw=dict(aspect=\"equal\"))\nfig.suptitle('Proportion of Customers Who Have Personal Loan and Who Don\\'t, \\\namong Different Family Sizes', fontsize = 16, y = 1.1, x = 0.51);\n\nax1.pie(series_fam_3, labels = labels, autopct= '%1.1f%%', shadow = True,explode = (0, 0.1), radius = 1.25, startangle = 90)\nax1.set_title('Family Size 3',fontsize = 14, y = 1.1)\n\nax2.pie(series_fam_4, labels = labels, autopct= '%1.1f%%', shadow = True,explode = (0, 0.1), radius = 1.25, startangle = 90)\nax2.set_title('Family Size 4', fontsize = 14, y = 1.1);\n\nplt.savefig('Proportion_of_PL_among_family_levels1.png', bbox_inches = 'tight');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.axis('equal')\nplt.title('Proportion of Customers With Different Family Sizes \\n among Personal Loan Holders', \\\n          fontsize = 14, y = 1.3)\nlabels = ['Family 2',' Family 1','Family 3','Family 4']\nplt.pie(series_fam.sort_values(ascending = True), labels = labels, \\\n        autopct= '%1.2f%%', shadow = True, explode = (0.1, 0.1, 0.1,0.15), radius = 1.6, startangle = 90);\nplt.savefig('Proportion_family_size_among_PL1.png', bbox_inches = 'tight');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Conclusion\n\n    27.9% and 27.7% of persons who have Personal Loan, have Family size 4 and Level 3 respectively.\n    \n    For 'Family' characteristic - the main segments to sell Personal Loan is the people who have Family Size 3 and 4.\n    \n    Target values of 'Family' variable are 3 and 4 in descending order of priority, since the proportion of people who has Personal Loan is the higthest with Family Size 3 - 13,2%.","metadata":{}},{"cell_type":"markdown","source":"#### CCAvg\n\nHere is the distribution of \"CCAvg\" values among Personal Loan holders and among whole population.","metadata":{}},{"cell_type":"code","source":"series_cca = exp_df[exp_df['Personal Loan'] == 1]['CCAvg'].value_counts()\nseries_cca","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_cca.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"width = 1.5 #wdth of bins in histogram - play with it to find good point for groupping\nseries_cca.plot.hist(bins = np.arange(series_cca.min(), series_cca.max() + width, width ), figsize = (8,6))\nplt.xlabel('CCAvg')\nplt.axvline(x = series_cca.mean(), color = 'red')\nplt.axvline(x = series_cca.min(), color = 'green')\nplt.axvline(x = series_cca.mean() + series_cca.std(), color = 'green')\nplt.title('Distribution of \"CCAvg\" values among \"Personal Loan\" holders', fontsize = 14, y = 1.05);\nplt.savefig('Distrib_ccavg_among_PL1.png', bbox_inches = 'tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\n\nWe may say that CCAvg characteristics values can be devided in three groups in descending order of priority consider its frequncy among Personal Loan holder:\n\n    Group I: 1 < CCAvg < 2.5\n    Group II: 4 < CCAvg < 5.5\n    Group III: 7 < CCAvg < 8.5","metadata":{}},{"cell_type":"code","source":"series_ccaa = exp_df['CCAvg'].value_counts()\nwidth = 8.5 #wdth of bins in histogram - play with it to find good point for groupping\nseries_ccaa.plot.hist(bins = np.arange(series_ccaa.min(), series_ccaa.max() + width, width ), figsize = (8,6))\nplt.xlabel('CCAvg')\nplt.title('Distribution of \"CCAvg\" values among whole population', fontsize = 14, y = 1.05);\nplt.savefig('Distrib_ccavg_among_population1.png', bbox_inches = 'tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\nWe may say, that all our groups of 'CCAvg' defined as priority groups to sell Personal Loan, lies inside segment with pretty high frequency among whole population.\n\n##### Conclusion\n\nTarget groups of 'CCAvg' characteristic is in descending order of priority:\n\n    Group I: 1 < CCAvg < 2.5\n    Group II: 4 < CCAvg < 5.5\n    Group III: 7 < CCAvg < 8.5\n","metadata":{}},{"cell_type":"markdown","source":"#### Income\n\nHere is the distribution of \"Income\" values among Personal Loan holders and among whole population","metadata":{}},{"cell_type":"code","source":"series_inc = exp_df[exp_df['Personal Loan'] == 1]['Income'].value_counts()\nseries_inc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_inc.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"width = 1.5 #wdth of bins in histogram - play with it to find good point for groupping\nseries_inc.plot.hist(bins = np.arange(series_inc.min(), series_inc.max() + width, width ), figsize = (8,6))\nplt.xlabel('Income')\nplt.axvline(x = series_inc.mean(), color = 'red')\nplt.axvline(x = series_inc.min(), color = 'green')\nplt.axvline(x = series_inc.mean() + series_inc.std(), color = 'green')\nplt.title('Distribution of \"Income\" values among \"Personal Loan\" holders', fontsize = 14, y = 1.05);\nplt.savefig('Distrib_income_among_PL1.png', bbox_inches = 'tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\n\nWe may say that 'Income' characteristic values can be devided in three groups in descending order of priority consider its frequncy among Personal Loan holder:\n\n    Group I: 1 < Income < 2.5\n    Group II: 4 < Income < 5.5\n    Group III: 7 < Income < 8.5","metadata":{}},{"cell_type":"code","source":"series_incc = exp_df['Income'].value_counts()\nwidth = 8.5 #wdth of bins in histogram - play with it to find good point for groupping\nseries_incc.plot.hist(bins = np.arange(series_incc.min(), series_incc.max() + width, width ), figsize = (8,6))\nplt.xlabel('Income')\nplt.title('Distribution of \"Income\" values among whole population', fontsize = 14, y = 1.05);\nplt.savefig('Distrib_income_among_population1.png', bbox_inches = 'tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observation\nWe may say, that all our groups of 'Income' defined as priority groups to sell Personal Loan, lies inside segment with pretty high frequency among whole population.\n\n##### Conclusion\n\nTarget groups of 'Income' characteristic is:\n\n    Group I: 1 < Income < 2.5\n    Group II: 4 < Income < 5.5\n    Group III: 7 < Income < 8.5","metadata":{}},{"cell_type":"markdown","source":"##### Summary Conclusion\n\nWe made the simple step-by-step analysis of customer's characteristics to identify patterns to effectively choose the subset of customers who have a higher probability to buy new product \"Personal Loan\" from The Bank. We performed the following steps:\n\n    We check all twelve characteristics whether or not each of them has an association with the fact the product been sold.\n    We find FIVE main characteristics that have higher than moderate strength of association with the product.\n    We analyze main characteristics and get segments in each with different strength of association with the product.","metadata":{}},{"cell_type":"markdown","source":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","metadata":{}}]}